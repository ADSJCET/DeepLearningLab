{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y2JndOg_KTY",
        "outputId": "329653e2-46c7-457e-a520-9f4c268e93af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 6s/step - loss: 7.6503 - val_loss: 5.8158\n",
            "Epoch 2/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 6s/step - loss: 5.6480 - val_loss: 5.5949\n",
            "Epoch 3/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 6s/step - loss: 5.3670 - val_loss: 5.4387\n",
            "Epoch 4/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 6s/step - loss: 5.1576 - val_loss: 5.3511\n",
            "Epoch 5/5\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 6s/step - loss: 4.9928 - val_loss: 5.2681\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Input English sentence: which is a pity but in india every other sport\n",
            "Predicted Hindi translation: और हम एक तरह के लिए एक तरह से\n",
            "Actual Hindi sentence: START_ जिसपे हमें तरस आती है लेकिन भारत में हर खेल _END\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('dataset.csv', encoding='utf-8')\n",
        "\n",
        "# Get English and Hindi vocabularies\n",
        "all_eng_words = set()\n",
        "for eng in data['English']:\n",
        "    for word in eng.split():\n",
        "        all_eng_words.add(word)\n",
        "\n",
        "all_hin_words = set()\n",
        "for hin in data['Hindi']:\n",
        "    for word in hin.split():\n",
        "        all_hin_words.add(word)\n",
        "\n",
        "# Create sentence length columns\n",
        "data['len_eng_sen'] = data['English'].apply(lambda x: len(x.split(\" \")))\n",
        "data['len_hin_sen'] = data['Hindi'].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "# Filter sentences by max length (limit both Hindi and English to 20 tokens)\n",
        "data = data[data['len_eng_sen'] <= 20]\n",
        "data = data[data['len_hin_sen'] <= 20]\n",
        "\n",
        "# Get the maximum length of the sentences\n",
        "max_len_src = max(data['len_eng_sen'])\n",
        "max_len_tar = max(data['len_hin_sen'])\n",
        "\n",
        "# Prepare vocabulary and token index mappings\n",
        "inp_words = sorted(list(all_eng_words))\n",
        "tar_words = sorted(list(all_hin_words))\n",
        "\n",
        "num_enc_toks = len(inp_words) + 1  # +1 for padding index (0)\n",
        "num_dec_toks = len(tar_words) + 1  # +1 for zero padding\n",
        "\n",
        "# Create token-to-index and index-to-token mappings\n",
        "inp_tok_idx = dict((word, i + 1) for i, word in enumerate(inp_words))  # Input word -> index\n",
        "tar_tok_idx = dict((word, i + 1) for i, word in enumerate(tar_words))  # Target word -> index\n",
        "rev_inp_tok_idx = dict((i, word) for word, i in inp_tok_idx.items())    # Index -> Input word\n",
        "rev_tar_tok_idx = dict((i, word) for word, i in tar_tok_idx.items())    # Index -> Target word\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X, y = data['English'], data['Hindi']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Padding sequences to ensure all sentences have the same length\n",
        "X_train_padded = pad_sequences(X_train.apply(lambda x: [inp_tok_idx.get(word, 0) for word in x.split()]), maxlen=max_len_src, padding='post')\n",
        "y_train_padded = pad_sequences(y_train.apply(lambda x: [tar_tok_idx.get(word, 0) for word in x.split()]), maxlen=max_len_tar, padding='post')\n",
        "\n",
        "X_test_padded = pad_sequences(X_test.apply(lambda x: [inp_tok_idx.get(word, 0) for word in x.split()]), maxlen=max_len_src, padding='post')\n",
        "y_test_padded = pad_sequences(y_test.apply(lambda x: [tar_tok_idx.get(word, 0) for word in x.split()]), maxlen=max_len_tar, padding='post')\n",
        "\n",
        "# Create a TensorFlow dataset using from_generator\n",
        "def generate_batch(X, y, batch_size=batch_size):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            enc_inp_data = np.zeros((batch_size, max_len_src), dtype='float32')\n",
        "            dec_inp_data = np.zeros((batch_size, max_len_tar), dtype='float32')\n",
        "            dec_tar_data = np.zeros((batch_size, max_len_tar, num_dec_toks), dtype='float32')\n",
        "\n",
        "            for i, (inp_seq, tar_seq) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):\n",
        "                enc_inp_data[i, :len(inp_seq)] = inp_seq  # Encoder input data\n",
        "                for t, tok in enumerate(tar_seq):\n",
        "                    if t < len(tar_seq) - 1:\n",
        "                        dec_inp_data[i, t] = tok  # Decoder input (shifted target sequence)\n",
        "                    if t > 0:\n",
        "                        dec_tar_data[i, t - 1, tok] = 1.0  # Decoder target (one-hot encoded)\n",
        "\n",
        "            # Convert to tensors to match the output signature\n",
        "            yield (tf.convert_to_tensor(enc_inp_data, dtype=tf.float32),\n",
        "                   tf.convert_to_tensor(dec_inp_data, dtype=tf.float32)), tf.convert_to_tensor(dec_tar_data, dtype=tf.float32)\n",
        "\n",
        "# Use tf.data.Dataset to wrap the generator\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batch(X_train_padded, y_train_padded, batch_size=batch_size),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None, max_len_src), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(None, max_len_tar), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(None, max_len_tar, num_dec_toks), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batch(X_test_padded, y_test_padded, batch_size=batch_size),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None, max_len_src), dtype=tf.float32),\n",
        "         tf.TensorSpec(shape=(None, max_len_tar), dtype=tf.float32)),\n",
        "        tf.TensorSpec(shape=(None, max_len_tar, num_dec_toks), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Model Architecture\n",
        "latent_dim = 250\n",
        "\n",
        "# Encoder\n",
        "enc_inps = Input(shape=(None,))\n",
        "enc_emb = Embedding(num_enc_toks, latent_dim, mask_zero=True)(enc_inps)\n",
        "enc_lstm = LSTM(latent_dim, return_state=True)\n",
        "enc_outputs, st_h, st_c = enc_lstm(enc_emb)\n",
        "enc_states = [st_h, st_c]\n",
        "\n",
        "# Decoder\n",
        "dec_inps = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_dec_toks, latent_dim, mask_zero=True)\n",
        "dec_emb = dec_emb_layer(dec_inps)\n",
        "dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_emb, initial_state=enc_states)\n",
        "dec_dense = Dense(num_dec_toks, activation='softmax')\n",
        "dec_outputs = dec_dense(dec_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([enc_inps, dec_inps], dec_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train the model with a larger batch size using tf.data.Dataset\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=len(X_train_padded) // batch_size,\n",
        "    epochs=5,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=len(X_test_padded) // batch_size\n",
        ")\n",
        "\n",
        "# Inference Setup: Encoder\n",
        "enc_model = Model(enc_inps, enc_states)\n",
        "\n",
        "# Inference Setup: Decoder\n",
        "dec_st_inp_h = Input(shape=(latent_dim,))\n",
        "dec_st_inp_c = Input(shape=(latent_dim,))\n",
        "dec_states_inps = [dec_st_inp_h, dec_st_inp_c]\n",
        "\n",
        "dec_emb2 = dec_emb_layer(dec_inps)\n",
        "dec_outputs2, st_h2, st_c2 = dec_lstm(dec_emb2, initial_state=dec_states_inps)\n",
        "dec_states2 = [st_h2, st_c2]\n",
        "dec_outputs2 = dec_dense(dec_outputs2)\n",
        "\n",
        "# Final decoder model for inference\n",
        "dec_model = Model([dec_inps] + dec_states_inps, [dec_outputs2] + dec_states2)\n",
        "\n",
        "# Translate function for inference\n",
        "def translate(inp_seq):\n",
        "    states_value = enc_model.predict(inp_seq)\n",
        "    tar_seq = np.zeros((1, 1))\n",
        "    tar_seq[0, 0] = tar_tok_idx.get('START_', 0)  # 'START_' token\n",
        "\n",
        "    stop_cond = False\n",
        "    dec_sen = ''\n",
        "    while not stop_cond:\n",
        "        output_toks, h, c = dec_model.predict([tar_seq] + states_value)\n",
        "        sampled_tok_idx = np.argmax(output_toks[0, -1, :])\n",
        "        sampled_char = rev_tar_tok_idx.get(sampled_tok_idx, '')\n",
        "\n",
        "        if sampled_char == '_END' or len(dec_sen.split()) > max_len_tar:\n",
        "            stop_cond = True\n",
        "        else:\n",
        "            dec_sen += ' ' + sampled_char\n",
        "\n",
        "        tar_seq = np.zeros((1, 1))\n",
        "        tar_seq[0, 0] = sampled_tok_idx\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return dec_sen.strip()\n",
        "\n",
        "# Testing the model on a training example\n",
        "train_gen = generate_batch(X_train_padded, y_train_padded, batch_size=1)\n",
        "(inp_seq, actual_output), _ = next(train_gen)\n",
        "hin_sen = translate(inp_seq)\n",
        "\n",
        "k = 0\n",
        "print(f'''Input English sentence: {X_train.iloc[k]}\\nPredicted Hindi translation: {hin_sen}\\nActual Hindi sentence: {y_train.iloc[k]}''')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
